"""
å¢å¼ºç‰ˆé¡¹ç›®è¯¦æƒ…é¡µçˆ¬è™«
æ”¯æŒå®Œæ•´ä¿¡æ¯æå–ï¼Œå»é™¤ä¸éœ€è¦çš„å­—æ®µ
"""

import json
import time
import os
import threading
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed

from driver_pool import get_global_pool, close_global_pool
from digitaling_parser_enhanced import DigitalingEnhancedParser
from config_optimized import SCRAPER_CONFIG, DELAY_CONFIG, EXTRACTION_CONFIG, FILE_PATHS


@dataclass
class ProjectInfo:
    """ç®€åŒ–çš„é¡¹ç›®ä¿¡æ¯æ•°æ®ç»“æ„"""
    id: str
    url: str
    title: str
    brand: str = ""
    agency: str = ""
    publish_date: str = ""
    description: str = ""
    category: str = ""
    images: List[str] = None
    project_info: Dict[str, str] = None  # é¡¹ç›®ä¿¡æ¯åŒºåŸŸçš„å…¶ä»–ä¿¡æ¯
    scraped_at: str = ""
    
    def __post_init__(self):
        if self.images is None:
            self.images = []
        if self.project_info is None:
            self.project_info = {}
        if not self.scraped_at:
            self.scraped_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')


@dataclass
class BatchInfo:
    """æ‰¹æ¬¡ä¿¡æ¯"""
    batch_id: str
    start_time: str
    end_time: str = ""
    total_projects: int = 0
    success_count: int = 0
    error_count: int = 0
    failed_urls: List[str] = None
    
    def __post_init__(self):
        if self.failed_urls is None:
            self.failed_urls = []


class ProjectDetailScraperEnhanced:
    """å¢å¼ºç‰ˆé¡¹ç›®è¯¦æƒ…é¡µçˆ¬è™«"""
    
    def __init__(self, headless=True, max_workers=3, output_dir="output", pool_size=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
            output_dir: è¾“å‡ºç›®å½•
            pool_size: WebDriverè¿æ¥æ± å¤§å°ï¼ŒNoneåˆ™ä½¿ç”¨max_workers
        """
        self.headless = headless
        self.max_workers = max_workers
        self.output_dir = output_dir
        self.pool_size = pool_size or max_workers
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, "details"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
        
        # é…ç½®æ–‡ä»¶è·¯å¾„
        self.summary_file = os.path.join(output_dir, "projects_summary.json")
        self.metadata_file = os.path.join(output_dir, "metadata.json")
        self.progress_file = os.path.join(output_dir, "scrape_progress.json")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_projects = 0
        self.completed_projects = 0
        self.failed_projects = 0
        
        # æ‰¹æ¬¡ä¿¡æ¯
        self.current_batch = None
        self.batch_size = SCRAPER_CONFIG.get("batch_size", 100)
        
        # çº¿ç¨‹é”
        self.stats_lock = threading.Lock()
        
        # è·å–å…¨å±€WebDriverè¿æ¥æ± 
        self.driver_pool = get_global_pool(
            pool_size=self.pool_size, 
            headless=self.headless
        )
    
    def scrape_project_detail(self, url: str) -> Optional[ProjectInfo]:
        """
        æŠ“å–å•ä¸ªé¡¹ç›®è¯¦æƒ…ï¼ˆä½¿ç”¨è¿æ¥æ± å’Œå¢å¼ºè§£æå™¨ï¼‰
        
        Args:
            url: é¡¹ç›®è¯¦æƒ…é¡µURL
            
        Returns:
            é¡¹ç›®ä¿¡æ¯æˆ–None
        """
        try:
            # ä»è¿æ¥æ± è·å–WebDriver
            with self.driver_pool.get_driver() as driver:
                # åˆ›å»ºå¢å¼ºè§£æå™¨å®ä¾‹
                parser = DigitalingEnhancedParser(driver)
                
                # è§£æé¡¹ç›®è¯¦æƒ…
                project_data = parser.parse_project_detail(url)
                
                if not project_data:
                    return None
                
                # ä»project_infoä¸­æå–å­—æ®µï¼Œå¦‚æœä¸»å­—æ®µä¸ºç©ºçš„è¯
                project_info_data = project_data.get('project_info', {})
                
                # åˆ›å»ºProjectInfoå¯¹è±¡
                project = ProjectInfo(
                    id=project_data['id'],
                    url=project_data['url'],
                    title=project_data['title'],
                    brand=project_data.get('brand') or project_info_data.get('brand', ''),
                    agency=project_data.get('agency') or project_info_data.get('agency', ''),
                    publish_date=project_data.get('publish_date') or project_info_data.get('publish_date', ''),
                    description=project_data.get('description', ''),
                    category=project_info_data.get('category', ''),
                    images=project_data.get('images', []),
                    project_info=project_info_data  # ä¿å­˜å®Œæ•´çš„é¡¹ç›®ä¿¡æ¯åŒºåŸŸæ•°æ®
                )
                
                return project
                
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥ {url}: {e}")
            return None
    
    def load_urls_from_file(self, filename: str) -> List[str]:
        """ä»æ–‡ä»¶åŠ è½½URLåˆ—è¡¨"""
        urls = []
        
        if not os.path.exists(filename):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            return urls
        
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    urls.append(line)
        
        print(f"ä» {filename} åŠ è½½äº† {len(urls)} ä¸ªURL")
        return urls
    
    def load_existing_summary(self) -> dict:
        """åŠ è½½ç°æœ‰çš„æ±‡æ€»ä¿¡æ¯"""
        if os.path.exists(self.summary_file):
            try:
                with open(self.summary_file, 'r', encoding='utf-8') as f:
                    summary = json.load(f)
                print(f"åŠ è½½ç°æœ‰æ±‡æ€»ä¿¡æ¯: {len(summary.get('projects', []))} ä¸ªé¡¹ç›®")
                return summary
            except Exception as e:
                print(f"åŠ è½½ç°æœ‰æ±‡æ€»ä¿¡æ¯å¤±è´¥: {e}")
        
        return {
            "total_projects": 0,
            "total_batches": 0,
            "last_updated": "",
            "projects": []
        }
    
    def get_existing_urls(self) -> set:
        """è·å–å·²æŠ“å–çš„URLé›†åˆ"""
        summary = self.load_existing_summary()
        existing_urls = set()
        
        for project in summary.get('projects', []):
            if 'url' in project:
                existing_urls.add(project['url'])
        
        print(f"å·²æŠ“å– {len(existing_urls)} ä¸ªURL")
        return existing_urls
    
    def filter_new_urls(self, urls: List[str]) -> List[str]:
        """è¿‡æ»¤å‡ºæ–°çš„URL"""
        existing_urls = self.get_existing_urls()
        new_urls = [url for url in urls if url not in existing_urls]
        
        skipped_count = len(urls) - len(new_urls)
        if skipped_count > 0:
            print(f"è·³è¿‡ {skipped_count} ä¸ªå·²æŠ“å–çš„URL")
        
        return new_urls
    
    def save_batch_data(self, batch_info: BatchInfo, projects: List[ProjectInfo]):
        """ä¿å­˜æ‰¹æ¬¡æ•°æ®ï¼ˆå¢é‡ä¿å­˜ï¼‰"""
        # ç”Ÿæˆå”¯ä¸€çš„æ‰¹æ¬¡æ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        batch_file = os.path.join(self.output_dir, "details", f"projects_batch_{batch_info.batch_id}_{timestamp}.json")
        
        batch_data = {
            "batch_info": asdict(batch_info),
            "projects": [asdict(project) for project in projects]
        }
        
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(batch_data, f, ensure_ascii=False, indent=2)
        
        print(f"æ‰¹æ¬¡æ•°æ®å·²ä¿å­˜: {batch_file}")
        
        # åŒæ—¶æ›´æ–°æ‰¹æ¬¡ç´¢å¼•æ–‡ä»¶
        self.update_batch_index(batch_info.batch_id, batch_file, len(projects))
    
    def update_batch_index(self, batch_id: str, batch_file: str, project_count: int):
        """æ›´æ–°æ‰¹æ¬¡ç´¢å¼•æ–‡ä»¶"""
        index_file = os.path.join(self.output_dir, "details", "batch_index.json")
        
        # åŠ è½½ç°æœ‰ç´¢å¼•
        if os.path.exists(index_file):
            try:
                with open(index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
            except:
                index_data = {"batches": []}
        else:
            index_data = {"batches": []}
        
        # æ·»åŠ æ–°æ‰¹æ¬¡ä¿¡æ¯
        batch_info = {
            "batch_id": batch_id,
            "file_path": os.path.basename(batch_file),
            "project_count": project_count,
            "created_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒbatch_idçš„è®°å½•ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°
        existing_batch = None
        for i, batch in enumerate(index_data["batches"]):
            if batch["batch_id"] == batch_id:
                existing_batch = i
                break
        
        if existing_batch is not None:
            index_data["batches"][existing_batch] = batch_info
        else:
            index_data["batches"].append(batch_info)
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        print(f"æ‰¹æ¬¡ç´¢å¼•å·²æ›´æ–°: {index_file}")
    
    def update_summary(self, new_projects: List[ProjectInfo]):
        """æ›´æ–°æ±‡æ€»ä¿¡æ¯ï¼ˆåˆå¹¶ç°æœ‰æ•°æ®ï¼‰"""
        # åŠ è½½ç°æœ‰æ±‡æ€»ä¿¡æ¯
        existing_summary = self.load_existing_summary()
        existing_projects = existing_summary.get('projects', [])
        
        # åˆ›å»ºç°æœ‰é¡¹ç›®çš„URLé›†åˆï¼Œç”¨äºå»é‡
        existing_urls = {p.get('url', '') for p in existing_projects}
        
        # æ·»åŠ æ–°é¡¹ç›®ï¼ˆå»é‡ï¼‰
        added_count = 0
        for project in new_projects:
            if project.url not in existing_urls:
                project_summary = {
                    "id": project.id,
                    "url": project.url,
                    "title": project.title,
                    "brand": project.brand,
                    "agency": project.agency,
                    "publish_date": project.publish_date,
                    "category": project.category,
                    "description": project.description[:200] + "..." if len(project.description) > 200 else project.description,  # æ‘˜è¦
                    "image_count": len(project.images),
                    "batch_id": project.scraped_at.split(' ')[0].replace('-', ''),
                    "scraped_at": project.scraped_at
                }
                existing_projects.append(project_summary)
                existing_urls.add(project.url)
                added_count += 1
        
        # æ›´æ–°æ±‡æ€»ä¿¡æ¯
        summary_data = {
            "total_projects": len(existing_projects),
            "total_batches": (len(existing_projects) + self.batch_size - 1) // self.batch_size,
            "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "scraper_config": {
                "headless": self.headless,
                "max_workers": self.max_workers,
                "pool_size": self.pool_size,
                "batch_size": self.batch_size,
                "version": "enhanced-2.0"
            },
            "projects": existing_projects
        }
        
        with open(self.summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        print(f"æ±‡æ€»ä¿¡æ¯å·²æ›´æ–°: {self.summary_file}")
        print(f"  - æ–°å¢ {added_count} ä¸ªé¡¹ç›®")
        print(f"  - æ€»è®¡ {len(existing_projects)} ä¸ªé¡¹ç›®")
    
    def scrape_batch(self, urls: List[str], batch_id: str) -> Tuple[BatchInfo, List[ProjectInfo]]:
        """æŠ“å–ä¸€ä¸ªæ‰¹æ¬¡çš„é¡¹ç›®"""
        batch_info = BatchInfo(
            batch_id=batch_id,
            start_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            total_projects=len(urls)
        )
        
        projects = []
        
        # æ˜¾ç¤ºè¿æ¥æ± çŠ¶æ€
        pool_status = self.driver_pool.get_pool_status()
        print(f"WebDriverè¿æ¥æ± çŠ¶æ€: {pool_status}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æŠ“å–ï¼Œå¤ç”¨WebDriverè¿æ¥æ± 
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.scrape_project_detail, url): url for url in urls}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    project = future.result()
                    if project:
                        projects.append(project)
                        with self.stats_lock:
                            batch_info.success_count += 1
                        print(f"å®Œæˆ {batch_info.success_count}/{len(urls)}: {project.title[:40]}...")
                    else:
                        with self.stats_lock:
                            batch_info.error_count += 1
                            batch_info.failed_urls.append(url)
                        print(f"å¤±è´¥ {batch_info.error_count}: {url}")
                        
                except Exception as e:
                    print(f"æŠ“å–å¼‚å¸¸ {url}: {e}")
                    with self.stats_lock:
                        batch_info.error_count += 1
                        batch_info.failed_urls.append(url)
        
        batch_info.end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # æ˜¾ç¤ºæ‰¹æ¬¡ç»“æŸåçš„è¿æ¥æ± çŠ¶æ€
        pool_status = self.driver_pool.get_pool_status()
        print(f"æ‰¹æ¬¡å®Œæˆåè¿æ¥æ± çŠ¶æ€: {pool_status}")
        
        return batch_info, projects
    
    def run(self, urls_file: str, start_batch: int = 1, max_batches: int = None):
        """è¿è¡Œçˆ¬è™«"""
        print("=" * 60)
        print("å¢å¼ºç‰ˆé¡¹ç›®è¯¦æƒ…é¡µçˆ¬è™«å¯åŠ¨")
        print(f"âœ¨ æ–°åŠŸèƒ½: å®Œæ•´ä¿¡æ¯æå–ï¼Œç®€åŒ–æ•°æ®ç»“æ„")
        print(f"WebDriverè¿æ¥æ± å¤§å°: {self.pool_size}")
        print(f"æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°: {self.max_workers}")
        print("=" * 60)
        
        # åŠ è½½URLåˆ—è¡¨
        urls = self.load_urls_from_file(urls_file)
        if not urls:
            print("æ²¡æœ‰æ‰¾åˆ°è¦æŠ“å–çš„URL")
            return
        
        # è¿‡æ»¤æ–°URLï¼ˆå»é‡ï¼‰
        new_urls = self.filter_new_urls(urls)
        if not new_urls:
            print("æ‰€æœ‰URLéƒ½å·²æŠ“å–è¿‡ï¼Œæ— éœ€é‡å¤æŠ“å–")
            return
        
        print(f"éœ€è¦æŠ“å– {len(new_urls)} ä¸ªæ–°URL")
        
        try:
            all_new_projects = []
            total_batches = (len(new_urls) + self.batch_size - 1) // self.batch_size
            
            if max_batches:
                total_batches = min(total_batches, max_batches)
                print(f"é™åˆ¶æŠ“å–æ‰¹æ¬¡æ•°: {max_batches}")
            
            print(f"æ–°URLåˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡")
            print(f"ä»ç¬¬ {start_batch} æ‰¹æ¬¡å¼€å§‹æŠ“å–")
            
            for batch_num in range(start_batch, start_batch + total_batches):
                start_idx = (batch_num - 1) * self.batch_size
                end_idx = min(start_idx + self.batch_size, len(new_urls))
                batch_urls = new_urls[start_idx:end_idx]
                
                print(f"\n{'=' * 60}")
                print(f"å¼€å§‹æŠ“å–ç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡")
                print(f"æœ¬æ‰¹æ¬¡åŒ…å« {len(batch_urls)} ä¸ªé¡¹ç›®")
                print('=' * 60)
                
                # æŠ“å–æ‰¹æ¬¡
                batch_info, projects = self.scrape_batch(batch_urls, f"{batch_num:03d}")
                
                # ä¿å­˜æ‰¹æ¬¡æ•°æ®
                self.save_batch_data(batch_info, projects)
                
                # æ›´æ–°ç»Ÿè®¡
                all_new_projects.extend(projects)
                self.completed_projects += batch_info.success_count
                self.failed_projects += batch_info.error_count
                
                print(f"\næ‰¹æ¬¡ {batch_num} å®Œæˆ:")
                print(f"  æˆåŠŸ: {batch_info.success_count}")
                print(f"  å¤±è´¥: {batch_info.error_count}")
                print(f"  è€—æ—¶: {self._calculate_batch_duration(batch_info)}")
                
                # æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯æå–æ•ˆæœ
                if projects:
                    sample_project = projects[0]
                    print(f"  ğŸ“Š ä¿¡æ¯æå–ç¤ºä¾‹:")
                    print(f"    æ ‡é¢˜: {sample_project.title[:30]}...")
                    print(f"    å“ç‰Œ: {sample_project.brand or 'æœªè¯†åˆ«'}")
                    print(f"    ä»£ç†å•†: {sample_project.agency or 'æœªè¯†åˆ«'}")
                    print(f"    æè¿°é•¿åº¦: {len(sample_project.description)} å­—ç¬¦")
                    print(f"    å›¾ç‰‡æ•°é‡: {len(sample_project.images)} å¼ ")
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                if batch_num < start_batch + total_batches - 1:
                    delay = DELAY_CONFIG.get("batch_delay", 10)
                    print(f"ç­‰å¾…{delay}ç§’åç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡...")
                    time.sleep(delay)
            
            # æ›´æ–°æ±‡æ€»ä¿¡æ¯ï¼ˆåˆå¹¶ç°æœ‰æ•°æ®ï¼‰
            if all_new_projects:
                self.update_summary(all_new_projects)
            
            print(f"\n{'=' * 60}")
            print("å¢å¼ºç‰ˆæŠ“å–å®Œæˆï¼")
            print(f"æœ¬æ¬¡æ–°å¢: {len(all_new_projects)} ä¸ªé¡¹ç›®")
            print(f"æˆåŠŸ: {self.completed_projects}")
            print(f"å¤±è´¥: {self.failed_projects}")
            print(f"æˆåŠŸç‡: {self.completed_projects/(self.completed_projects+self.failed_projects)*100:.1f}%" if (self.completed_projects+self.failed_projects) > 0 else "N/A")
            print(f"ä¿¡æ¯å®Œæ•´æ€§å¤§å¹…æå‡")
            print('=' * 60)
            
        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
            print("æ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")
            # å³ä½¿ä¸­æ–­ä¹Ÿä¿å­˜å·²æŠ“å–çš„æ•°æ®
            if all_new_projects:
                self.update_summary(all_new_projects)
            print("è¿›åº¦å·²ä¿å­˜")
        
        except Exception as e:
            print(f"\nè¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            # ä¿å­˜å·²æŠ“å–çš„æ•°æ®
            if all_new_projects:
                self.update_summary(all_new_projects)
        
        finally:
            # ä¸éœ€è¦æ‰‹åŠ¨å…³é—­è¿æ¥æ± ï¼Œç”±å…¨å±€ç®¡ç†
            pass
    
    def _calculate_batch_duration(self, batch_info: BatchInfo) -> str:
        """è®¡ç®—æ‰¹æ¬¡æŒç»­æ—¶é—´"""
        try:
            start_time = datetime.strptime(batch_info.start_time, '%Y-%m-%d %H:%M:%S')
            end_time = datetime.strptime(batch_info.end_time, '%Y-%m-%d %H:%M:%S')
            duration = end_time - start_time
            
            minutes = int(duration.total_seconds() // 60)
            seconds = int(duration.total_seconds() % 60)
            
            if minutes > 0:
                return f"{minutes}åˆ†{seconds}ç§’"
            else:
                return f"{seconds}ç§’"
        except:
            return "æœªçŸ¥"
    
    def get_statistics(self) -> Dict:
        """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
        summary = self.load_existing_summary()
        pool_status = self.driver_pool.get_pool_status()
        
        return {
            "total_projects": summary.get("total_projects", 0),
            "total_batches": summary.get("total_batches", 0),
            "last_updated": summary.get("last_updated", ""),
            "current_session": {
                "completed": self.completed_projects,
                "failed": self.failed_projects,
                "success_rate": round(self.completed_projects/(self.completed_projects+self.failed_projects)*100, 2) if (self.completed_projects+self.failed_projects) > 0 else 0
            },
            "driver_pool": pool_status,
            "config": {
                "headless": self.headless,
                "max_workers": self.max_workers,
                "batch_size": self.batch_size,
                "pool_size": self.pool_size,
                "version": "enhanced-2.0"
            }
        }
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        # WebDriverè¿æ¥æ± ç”±å…¨å±€ç®¡ç†ï¼Œè¿™é‡Œä¸éœ€è¦æ‰‹åŠ¨å…³é—­
        pass


if __name__ == "__main__":
    # åˆ›å»ºå¢å¼ºç‰ˆçˆ¬è™«å®ä¾‹
    scraper = ProjectDetailScraperEnhanced(
        headless=SCRAPER_CONFIG.get("headless", True),
        max_workers=SCRAPER_CONFIG.get("max_workers", 2),
        output_dir=SCRAPER_CONFIG.get("output_dir", "output"),
        pool_size=3  # WebDriverè¿æ¥æ± å¤§å°
    )
    
    try:
        # è¿è¡Œçˆ¬è™«ï¼ˆéªŒè¯æ¨¡å¼ï¼‰
        scraper.run("project_urls.txt", start_batch=1, max_batches=1)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = scraper.get_statistics()
        print(f"\nç»Ÿè®¡ä¿¡æ¯: {json.dumps(stats, ensure_ascii=False, indent=2)}")
        
    except Exception as e:
        print(f"è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # å…³é—­å…¨å±€è¿æ¥æ± 
        close_global_pool()
        print("èµ„æºæ¸…ç†å®Œæˆ")